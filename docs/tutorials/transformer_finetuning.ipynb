{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Transformer Finetuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have previously shown how [Molfeat integrates with PyTorch in general](./integration.html) and even with [Pytorch Geometric](./pyg_integration.html). Now we will demonstrate how to use molfeat to finetune a pretrained transformer. This tutorial will walk you through an example of finetuning the ChemBERTa pretrained model for molecular property prediction. These same principles can be applied to any pretrained transformers available in molfeat.\n",
    "\n",
    "To run this tutorial, you will need to install `transformers` and `tokenizers`.\n",
    "\n",
    "`mamba install -c conda-forge transformers \"tokenizers <0.13.2\"`\n",
    "\n",
    "\n",
    "<div class=\"admonition warning highlight blink\">\n",
    "<p class=\"admonition-title\">Advanced users</p>\n",
    "<p>This tutorial is for advanced users that are comfortable with the APIs of molfeat and Hugging Face transformers.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molfeat.utils.converters import SmilesConverter\n",
    "from molfeat.trans.pretrained import PretrainedHFTransformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurizer\n",
    "\n",
    "Pretrained Transformer Featurizer in molfeat have an underlying object `featurizer` that can handle both tokenization and embedding. \n",
    "\n",
    "We will leverage this structure in molfeat to initialize our transformer model, but also to tokenize our molecules\n",
    "\n",
    "We first start by defining our featurizer. Here we will use the ChemBERTa pretrained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = PretrainedHFTransformer(kind=\"ChemBERTa-77M-MLM\", pooling=\"bert\", preload=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note the use of preload to preload the model in the `__init__`\n",
    "* Note how we define a pooling mechanism here. Molfeat provides [several poolers that you can explore in the API](./api/molfeat.utils.html#pooling). Because a pooling layer can already be specified and will be accessible through the `_pooling_obj` attribute we will not bother defining one later. Instead we will just retrieve the one from the featurizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "For the dataset, we will use the `BBBP` dataset, which contains binary labels of blood-brain barrier penetration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>name</th>\n",
       "      <th>p_np</th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Propanolol</td>\n",
       "      <td>1</td>\n",
       "      <td>[Cl].CC(C)NCC(O)COc1cccc2ccccc12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Terbutylchlorambucil</td>\n",
       "      <td>1</td>\n",
       "      <td>C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>40730</td>\n",
       "      <td>1</td>\n",
       "      <td>c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>cloxacillin</td>\n",
       "      <td>1</td>\n",
       "      <td>Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num                  name  p_np  \\\n",
       "0    1            Propanolol     1   \n",
       "1    2  Terbutylchlorambucil     1   \n",
       "2    3                 40730     1   \n",
       "3    4                    24     1   \n",
       "4    5           cloxacillin     1   \n",
       "\n",
       "                                              smiles  \n",
       "0                   [Cl].CC(C)NCC(O)COc1cccc2ccccc12  \n",
       "1           C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl  \n",
       "2  c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO...  \n",
       "3                   C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C  \n",
       "4  Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to define our PyTorch Dataset. As discussed above, we will leverage the internal structure of our transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "\n",
    "class DTset(Dataset):\n",
    "    def __init__(self, smiles, y, mf_featurizer):\n",
    "        super().__init__()\n",
    "        self.smiles = smiles\n",
    "        self.mf_featurizer = mf_featurizer\n",
    "        self.y = torch.tensor(y).float()\n",
    "        # here we use the molfeat mf_featurizer to convert the smiles to\n",
    "        # corresponding tokens based on the internal tokenizer\n",
    "        # we just want the data from the batch encoding object\n",
    "        self.transformed_mols = self.mf_featurizer._convert(smiles)\n",
    "\n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        return len(self.mf_featurizer)\n",
    "\n",
    "    @property\n",
    "    def max_length(self):\n",
    "        return self.transformed_mols.shape[-1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def collate_fn(self, **kwargs):\n",
    "        # the default collate fn self.mf_featurizer.get_collate_fn(**kwargs)\n",
    "        # returns None, which should just concatenate the inputs\n",
    "        # You could also use `transformers.default_data_collator` instead\n",
    "        return self.mf_featurizer.get_collate_fn(**kwargs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        datapoint = dict((name, val[index]) for name, val in self.transformed_mols.items())\n",
    "        datapoint[\"y\"] = self.y[index]\n",
    "        return datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a116e0dc9d44664ba66ee757a433087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2050 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d207a8623e4b4dbd0d1b81a2a472e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2050 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = DTset(df.smiles.values, df.p_np.values, featurizer)\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dt, test_dt = torch.utils.data.random_split(dataset, [0.8, 0.2], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dt, batch_size=BATCH_SIZE, shuffle=True, collate_fn=dataset.collate_fn())\n",
    "test_loader = DataLoader(test_dt, batch_size=BATCH_SIZE, shuffle=False, collate_fn=dataset.collate_fn())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network + Training\n",
    "We are ready to go, now we just need to define our Model for finetuning pretrained ChemBerta on the BBBP task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AwesomeNet(torch.nn.Module):\n",
    "    def __init__(self, mf_featurizer, hidden_size=128, dropout=0.1, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # we get the underlying model from the molfeat featurizer\n",
    "        # here we fetch the \"base\" huggingface transformer model \n",
    "        # and not the wrapper around for MLM\n",
    "        # this is principally to get smaller model and training efficiency\n",
    "        base_pretrained_model = getattr(mf_featurizer.featurizer.model, mf_featurizer.featurizer.model.base_model_prefix)\n",
    "        self.embedding_layer = copy.deepcopy(base_pretrained_model)\n",
    "        self.embedding_dim = mf_featurizer.featurizer.model.config.hidden_size\n",
    "        # given that we are not concatenating layers, the following is equivalent\n",
    "        # self.embedding_dim = len(mf_featurizer)\n",
    "        # we get the the pooling layer from the molfeat featurizer\n",
    "        self.pooling_layer = mf_featurizer._pooling_obj\n",
    "        self.hidden_layer = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=dropout),\n",
    "            torch.nn.Linear(len(mf_featurizer), self.hidden_size),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.output_layer = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, *, y=None, **kwargs):\n",
    "        # get embeddings\n",
    "        x = self.embedding_layer(**kwargs)\n",
    "        # we take the last hidden state\n",
    "        # you could also set `output_hidden_states` to true above \n",
    "        # and take x[\"hidden_states\"][-1] instead\n",
    "        emb = x[\"last_hidden_state\"]\n",
    "        # run poolings\n",
    "        h = self.pooling_layer(\n",
    "            emb,\n",
    "            kwargs[\"input_ids\"],\n",
    "            mask=kwargs.get('attention_mask'),\n",
    "        )\n",
    "        # run through our custom and optional hidden layer\n",
    "        h = self.hidden_layer(h)\n",
    "        # run through output layers to get logits\n",
    "        return self.output_layer(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "PNA_AGGREGATORS = ['mean', 'min', 'max', 'std']\n",
    "PNA_SCALERS = ['identity', 'amplification', 'attenuation']\n",
    "\n",
    "model = AwesomeNet(featurizer, hidden_size=64, dropout=0.1, output_size=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE).float()\n",
    "model = model.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to have a look at the model summary for a sanity check.\n",
    "\n",
    "```python\n",
    "! pip install torchinfo\n",
    "from torchinfo import summary\n",
    "summary(model)\n",
    "```\n",
    "You should now see the following output:\n",
    "```\n",
    "==========================================================================================\n",
    "Layer (type:depth-idx)                                            Param #\n",
    "==========================================================================================\n",
    "AwesomeNet                                                        --\n",
    "├─RobertaForMaskedLM: 1-1                                         --\n",
    "│    └─RobertaModel: 2-1                                          --\n",
    "│    │    └─RobertaEmbeddings: 3-1                                429,312\n",
    "│    │    └─RobertaEncoder: 3-2                                   2,850,288\n",
    "│    └─RobertaLMHead: 2-2                                         --\n",
    "│    │    └─Linear: 3-3                                           147,840\n",
    "│    │    └─LayerNorm: 3-4                                        768\n",
    "│    │    └─Linear: 3-5                                           231,000\n",
    "├─BertPooler: 1-2                                                 --\n",
    "│    └─Linear: 2-3                                                147,840\n",
    "│    └─Tanh: 2-4                                                  --\n",
    "├─Sequential: 1-3                                                 --\n",
    "│    └─Dropout: 2-5                                               --\n",
    "│    └─Linear: 2-6                                                24,640\n",
    "│    └─ReLU: 2-7                                                  --\n",
    "├─Linear: 1-4                                                     65\n",
    "==========================================================================================\n",
    "Total params: 3,831,753\n",
    "Trainable params: 3,831,753\n",
    "Non-trainable params: 0\n",
    "==========================================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765e7f82f435427f8467b00387fd21c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train\n",
    "with tqdm(range(NUM_EPOCHS)) as pbar:\n",
    "    for epoch in pbar:\n",
    "        losses = []\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(**data)\n",
    "            loss = loss_fn(out.squeeze(), data[\"y\"])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        pbar.set_description(f\"Epoch {epoch} - Loss {np.mean(losses):.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "We can now test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.964\n",
      "Test Accuracy: 0.905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "test_y_hat = []\n",
    "test_y_true = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        out = model(**data)\n",
    "        # we apply sigmoid\n",
    "        out = torch.sigmoid(out)\n",
    "        test_y_hat.append(out.detach().cpu().squeeze())\n",
    "        test_y_true.append(data[\"y\"])\n",
    "test_y_hat = torch.cat(test_y_hat).squeeze().numpy()\n",
    "test_y_true = torch.cat(test_y_true).squeeze().numpy()\n",
    "roc_auc = roc_auc_score(test_y_true, test_y_hat)\n",
    "acc = accuracy_score(test_y_true, test_y_hat>=0.5)\n",
    "print(f\"Test ROC AUC: {roc_auc:.3f}\\nTest Accuracy: {acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25cc46219716499d7b4b65986f7c346bc1ce4d9f3ea6745d62d4866f9ff86342"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
