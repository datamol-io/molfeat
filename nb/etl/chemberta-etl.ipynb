{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molfeat.trans.pretrained.hf_transformers import HFExperiment\n",
    "from molfeat.trans.pretrained.hf_transformers import HFModel\n",
    "from molfeat.store import ModelInfo\n",
    "from molfeat.store import ModelStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelInfo(name='gin_supervised_contextpred', inputs='smiles', type='pretrained', version=0, group='dgllife', submitter='Datamol', description='GIN neural network model pre-trained with supervised learning and context prediction on molecules from ChEMBL.', representation='graph', require_3D=False, tags=['GIN', 'dgl', 'pytorch', 'graph'], authors=['Weihua Hu', 'Bowen Liu', 'Joseph Gomes', 'Marinka Zitnik', 'Percy Liang', 'Vijay Pande', 'Jure Leskovec'], reference='https://arxiv.org/abs/1905.12265', created_at=datetime.datetime(2023, 2, 2, 19, 51, 17, 228390), sha256sum='72dc062936b78b515ed5d0989f909ab7612496d698415d73826b974c9171504a'),\n",
       " ModelInfo(name='gin_supervised_edgepred', inputs='smiles', type='pretrained', version=0, group='dgllife', submitter='Datamol', description='GIN neural network model pre-trained with supervised learning and edge prediction on molecules from ChEMBL.', representation='graph', require_3D=False, tags=['GIN', 'dgl', 'pytorch', 'graph'], authors=['Weihua Hu', 'Bowen Liu', 'Joseph Gomes', 'Marinka Zitnik', 'Percy Liang', 'Vijay Pande', 'Jure Leskovec'], reference='https://arxiv.org/abs/1905.12265', created_at=datetime.datetime(2023, 2, 14, 17, 42, 4, 710823), sha256sum='c1198b37239c3b733f5b48cf265af4c3a1e8c448e2e26cb53e3517fd096213de'),\n",
       " ModelInfo(name='gin_supervised_infomax', inputs='smiles', type='pretrained', version=0, group='dgllife', submitter='Datamol', description='GIN neural network model pre-trained with mutual information maximisation on molecules from ChEMBL.', representation='graph', require_3D=False, tags=['GIN', 'dgl', 'pytorch', 'graph'], authors=['Weihua Hu', 'Bowen Liu', 'Joseph Gomes', 'Marinka Zitnik', 'Percy Liang', 'Vijay Pande', 'Jure Leskovec'], reference='https://arxiv.org/abs/1905.12265', created_at=datetime.datetime(2023, 2, 14, 17, 42, 6, 967631), sha256sum='78dc0f76cde2151f5aa403cbbffead0f24aeac4ce0b48dbfa2689e1a87b95216'),\n",
       " ModelInfo(name='gin_supervised_masking', inputs='smiles', type='pretrained', version=0, group='dgllife', submitter='Datamol', description='GIN neural network model pre-trained with masked modelling on molecules from ChEMBL.', representation='graph', require_3D=False, tags=['GIN', 'dgl', 'pytorch', 'graph'], authors=['Weihua Hu', 'Bowen Liu', 'Joseph Gomes', 'Marinka Zitnik', 'Percy Liang', 'Vijay Pande', 'Jure Leskovec'], reference='https://arxiv.org/abs/1905.12265', created_at=datetime.datetime(2023, 2, 14, 17, 42, 9, 221083), sha256sum='c1c797e18312ad44ff089159cb1ed79fd4c67b3d5673c562f61621d95a5d7632'),\n",
       " ModelInfo(name='jtvae_zinc_no_kl', inputs='smiles', type='pretrained', version=0, group='dgllife', submitter='Datamol', description='A JTVAE pre-trained on ZINC for molecule generation, without KL regularization', representation='other', require_3D=False, tags=['JTNN', 'JTVAE', 'dgl', 'pytorch', 'junction-tree', 'graph'], authors=['Wengong Jin', 'Regina Barzilay', 'Tommi Jaakkola'], reference='https://arxiv.org/abs/1802.04364v4', created_at=datetime.datetime(2023, 2, 2, 19, 51, 20, 468939), sha256sum='eab8ecb8a7542a8cdf97410cb27f72aaf374fefef6a1f53642cc5b310cf2b7f6'),\n",
       " ModelInfo(name='map4', inputs='smiles', type='hashed', version=0, group='fp', submitter='Datamol', description='MinHashed atom-pair fingerprint up to a diameter of four bonds (MAP4) is suitable for both small and large molecules by combining substructure and atom-pair concepts. In this fingerprint the circular substructures with radii of r\\u2009=\\u20091 and r\\u2009=\\u20092 bonds around each atom in an atom-pair are written as two pairs of SMILES, each pair being combined with the topological distance separating the two central atoms. These so-called atom-pair molecular shingles are hashed, and the resulting set of hashes is MinHashed to form the MAP4 fingerprint.', representation='vector', require_3D=False, tags=['minhashed', 'map4', 'atompair', 'substructure', 'morgan'], authors=['Alice Capecchi', 'Daniel Probst', 'Jean-Louis Reymond'], reference='https://doi.org/10.1186/s13321-020-00445-4', created_at=datetime.datetime(2023, 2, 16, 10, 29, 8, 550063), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1'),\n",
       " ModelInfo(name='secfp', inputs='smiles', type='hashed', version=0, group='fp', submitter='Datamol', description='SMILES extended connectivity fingerprint (SECFP), is a fingerprint variant on MinHash fingerprints (MHFPs) SMILES-based circular substructure hashing scheme, folded by the same modulo \\ud835\\udc5b operation that is used by ECFP.', representation='vector', require_3D=False, tags=['minhashed', 'smiles', 'ecfp', 'secfp', 'mhfp', 'mhfp6'], authors=['Daniel Probst', 'Jean-Louis Reymond'], reference='https://doi.org/10.1186/s13321-018-0321-8', created_at=datetime.datetime(2023, 2, 16, 10, 29, 11, 465320), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1'),\n",
       " ModelInfo(name='pcqm4mv2_graphormer_base', inputs='smiles', type='pretrained', version=0, group='graphormer', submitter='Datamol', description='Pretrained Graph Transformer on PCQM4Mv2 Homo-Lumo energy gap prediction using 2D molecular graphs.', representation='graph', require_3D=False, tags=['Graphormer', 'PCQM4Mv2', 'graph', 'pytorch', 'Microsoft'], authors=['Chengxuan Ying', 'Tianle Cai', 'Shengjie Luo', 'Shuxin Zheng', 'Guolin Ke', 'Di He', 'Yanming Shen', 'Tie-Yan Liu'], reference='https://arxiv.org/abs/2106.05234', created_at=datetime.datetime(2023, 2, 2, 19, 51, 19, 330147), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1'),\n",
       " ModelInfo(name='atompair-count', inputs='smiles', type='count', version=0, group='rdkit', submitter='Datamol', description='The Atompair-Count fingerprint is essentially the same as the atompair fingerprint. However, instead of being hashed into a binary vector, there is no hashing process and simply a count vector is returned', representation='vector', require_3D=False, tags=['atompaircount', 'interactions', 'frequency', 'rdkit', 'vector'], authors=['Raymond E. Carhart', 'Dennis H. Smith', 'R. Venkataraghavan'], reference='https://doi.org/10.1021/ci00046a002', created_at=datetime.datetime(2023, 2, 16, 10, 29, 36, 973090), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1'),\n",
       " ModelInfo(name='avalon', inputs='smiles', type='hashed', version=0, group='rdkit', submitter='Datamol', description='Similar to Daylight fingerprints, Avalon uses a fingerprint generator that enumerates certain paths and feature classes of the molecular graph.  The fingerprint bit positions are hashed from the description of the feature; however, the hash codes for all the path-style features are computed implicitly while they are enumerated.', representation='vector', require_3D=False, tags=['avalon', 'hashed', '2D', 'binary', 'rdkit', 'folded'], authors=['Peter Gedeck', 'Bernhard Rohde', 'Christian Bartels'], reference='https://doi.org/10.1021/ci050413p', created_at=datetime.datetime(2023, 2, 16, 10, 28, 42, 723089), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1'),\n",
       " ModelInfo(name='ecfp-count', inputs='smiles', type='count', version=0, group='rdkit', submitter='Datamol', description='The ECFP-Count (Extended Connectivity Fingerprints-Coun is essentially the same as the ECFP. However, instead of being hashed into a binary vector, there is no hashing process and simply a count vector is returned', representation='vector', require_3D=False, tags=['fixed', 'morgan', '2D', 'rdkit', 'ecfpcount', 'vector'], authors=['David Rogers', 'Mathew Hahn'], reference='https://doi.org/10.1021/ci100050t', created_at=datetime.datetime(2023, 2, 16, 10, 29, 27, 788409), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1'),\n",
       " ModelInfo(name='ecfp', inputs='smiles', type='hashed', version=0, group='rdkit', submitter='Datamol', description='Extended-connectivity fingerprints (ECFPs) are a family of circular fingerprints that are commonly used for the measure of molecular similarity. They are based on the connectivity of atoms in molecular graphs.', representation='vector', require_3D=False, tags=['fixed', 'morgan', '2D', 'binary', 'rdkit', 'ecfp', 'folded'], authors=['David Rogers', 'Mathew Hahn'], reference='https://doi.org/10.1021/ci100050t', created_at=datetime.datetime(2023, 2, 16, 10, 28, 46, 950355), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1'),\n",
       " ModelInfo(name='erg', inputs='smiles', type='hand-crafted', version=0, group='rdkit', submitter='Datamol', description='Extended Reduced Graph approach (ErG) describes a molecular structure by defining its pharmacophoric points and the topological distance between them. It uses a pairwise combination of pharmacophores and their distance to set a corresponding bit in a vector. The ErG fingerprint implements fuzzy incrementation, which favours retrieval of actives with different core structures (scaffold hopping).', representation='vector', require_3D=False, tags=['2D', 'pharmacophore', 'erg', 'graph', 'rdkit'], authors=['Nikolaus Stiefl', 'Ian A Watson', 'Knut Baumann', 'Andrea Zaliani'], reference='https://doi.org/10.1021/ci050457y', created_at=datetime.datetime(2023, 2, 16, 10, 29, 14, 308859), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1'),\n",
       " ModelInfo(name='estate', inputs='smiles', type='hand-crafted', version=0, group='rdkit', submitter='Datamol', description='Electrotopological state (Estate) indices are numerical values computed for each atom in a molecule, and which encode information about both the topological environment of that atom and the electronic interactions due to all other atoms in the molecule.', representation='vector', require_3D=False, tags=['electrotopological', 'electronic', 'interactions', 'estate', 'rdkit'], authors=['Lemont B. Kier', 'Lowell H. Hall'], reference='https://doi.org/10.1023/A:1015952613760', created_at=datetime.datetime(2023, 2, 16, 10, 29, 24, 838083), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1'),\n",
       " ModelInfo(name='fcfp-count', inputs='smiles', type='count', version=0, group='rdkit', submitter='Datamol', description='The FCFP-Count (Functional Class Fingerprints-Count) is essentially the same as the FCFP. However, instead of being hashed into a binary vector, there is no hashing process and simply a count vector is returned', representation='vector', require_3D=False, tags=['functional', 'fcfpcount', '2D', 'rdkit', 'pharmacophore'], authors=['David Rogers', 'Mathew Hahn'], reference='https://doi.org/10.1021/ci100050t', created_at=datetime.datetime(2023, 2, 16, 10, 29, 30, 814828), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1'),\n",
       " ModelInfo(name='fcfp', inputs='smiles', type='hashed', version=0, group='rdkit', submitter='Datamol', description='Functional-class fingerprints (FCFPs) are an extension of ECFPs which incorporate information about the functional classes of atoms in a molecule. FCFPs are intended to capture more abstract property-based substructural features and leverage atomic characteristics that relate more to pharmacophoric features (e.g. hydrogen donor/acceptor, polarity, aromaticity, etc.).', representation='vector', require_3D=False, tags=['functional', 'fcfp', '2D', 'binary', 'rdkit', 'pharmacophore', 'folded'], authors=['David Rogers', 'Mathew Hahn'], reference='https://doi.org/10.1021/ci100050t', created_at=datetime.datetime(2023, 2, 16, 10, 28, 50, 104228), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1'),\n",
       " ModelInfo(name='maccs', inputs='smiles', type='hand-crafted', version=0, group='rdkit', submitter='Datamol', description='MACCS keys are 166-bit 2D structure fingerprints that are commonly used for the measure of molecular similarity. They described the presence of key features in molecular graphs', representation='vector', require_3D=False, tags=['maccs', 'fixed', '2D', 'binary', 'rdkit'], authors=['MDL Information Systems'], reference='https://doi.org/10.1021/ci010132r', created_at=datetime.datetime(2023, 2, 2, 19, 51, 10, 688803), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1'),\n",
       " ModelInfo(name='pattern', inputs='smiles', type='hashed', version=0, group='rdkit', submitter='Datamol', description='Pattern fingerprints were designed to be used in substructure screening. The algorithm identifies features in the molecule by doing substructure searches using a small number of very generic SMARTS patterns and then hashing each occurrence of a pattern based on the atom and bond types involved. The fact that a particular pattern matched the molecule at all is also stored by hashing the pattern ID and size.', representation='vector', require_3D=False, tags=['pattern', 'fingerprints', 'rdkit', 'binary', 'predefined', 'substructures'], authors=['RDKit'], reference='https://www.rdkit.org/docs/RDKit_Book.html#pattern-fingerprints', created_at=datetime.datetime(2023, 2, 16, 10, 29, 5, 566897), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1'),\n",
       " ModelInfo(name='rdkit', inputs='smiles', type='hashed', version=0, group='rdkit', submitter='Datamol', description='This is an RDKit-specific fingerprint that is inspired by (though it differs significantly from) public descriptions of the Daylight fingerprint. The fingerprinting algorithm identifies all subgraphs in the molecule within a particular range of sizes, hashes each subgraph to generate a raw bit ID, that is then folded into the requested fingerprint size as binary vectors. Options are available to generate count-based forms of the fingerprint or “non-folded” forms (using a sparse representation).', representation='vector', require_3D=False, tags=['fingerprints', 'rdkit', 'binary', 'folded', 'daylight'], authors=['RDKit'], reference='https://www.rdkit.org/docs/RDKit_Book.html#rdkit-fingerprints', created_at=datetime.datetime(2023, 2, 16, 10, 28, 59, 610866), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1'),\n",
       " ModelInfo(name='topological-count', inputs='smiles', type='count', version=0, group='rdkit', submitter='Datamol', description='The Topological-Count fingerprint is essentially the same as the Topological fingerprint. However, instead of being hashed into a binary vector, there is no hashing process and simply a count vector is returned', representation='vector', require_3D=False, tags=['graph', 'topologicalcount', 'torsion', 'rdkit', 'vector'], authors=['Ramaswamy Nilakantan', 'Norman Bauman', 'J. Scott Dixon', 'R. Venkataraghavan'], reference='https://doi.org/10.1021/ci00054a008', created_at=datetime.datetime(2023, 2, 16, 10, 29, 33, 828325), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1'),\n",
       " ModelInfo(name='topological', inputs='smiles', type='hashed', version=0, group='rdkit', submitter='Datamol', description='Topological torsion fingerprints are a type of molecular fingerprint that represents the topological features of a molecule based on its graph representation. They are generated by computing the frequencies of all possible molecular torsions in a molecule and then encoding them as a binary vector.', representation='vector', require_3D=False, tags=['graph', 'topological', 'torsion', 'rdkit', 'binary', 'folded'], authors=['Ramaswamy Nilakantan', 'Norman Bauman', 'J. Scott Dixon', 'R. Venkataraghavan'], reference='https://doi.org/10.1021/ci00054a008', created_at=datetime.datetime(2023, 2, 16, 10, 28, 56, 506694), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store = ModelStore()\n",
    "store.available_models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Pretrained Model saving"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ChemBerta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemberta_77_mlm_card = ModelInfo(\n",
    "    name = \"ChemBERTa-77M-MLM\",\n",
    "    inputs = \"smiles\",\n",
    "    type=\"pretrained\",\n",
    "    group=\"huggingface\",\n",
    "    version=0,\n",
    "    submitter=\"Datamol\",\n",
    "    description=\"ChemBERTa is a pre-trained language model for molecules based on (Ro)BERT(a) trained on PubChem compounds. The MTR version was pretrained using mutitask regression objective, while the MLM version was pretrained using a masked language modeling objective\",\n",
    "    representation=\"line-notation\",\n",
    "    require_3D=False,\n",
    "    tags = [\"ChemBERTa-2\", \"smiles\",  'huggingface', \"transformers\", \"MLM\", \"RoBERTa\", \"PubChem\"],\n",
    "    authors= [\"Walid Ahmad\", \"Elana Simon\", \"Seyone Chithrananda\", \"Gabriel Grand\", \"Bharath Ramsundar\"],\n",
    "    reference = \"https://arxiv.org/abs/2209.01712\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05aa9a90387c4e178d1229546bccac31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff515fc7b3242e5bab8163126b8039c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/13.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73dd5a2eb2554fd483bf2baf805f420e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a497e1aa3e948698fcb33e6a561964e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/6.96k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b801b4680b2148c3b3c16deaf9ced18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a91dae24a1454b9133c9d6088b9226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/8.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6865a155f04425b72d3638be695ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc2414dcda24840a57745cce7de8895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/420 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e8b485270f4999a124a5b91c324c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 15:35:24.459 | INFO     | molfeat.trans.pretrained.hf_transformers:save:50 - Model saved to gs://molfeat-store-prod/artifacts/huggingface/ChemBERTa-77M-MLM/0/model.save\n",
      "2023-03-20 15:35:27.493 | INFO     | molfeat.store.modelstore:register:124 - Successfuly registered model ChemBERTa-77M-MLM !\n"
     ]
    }
   ],
   "source": [
    "# attempt to register the model\n",
    "model = HFModel.register_pretrained(\"DeepChem/ChemBERTa-77M-MLM\", \"DeepChem/ChemBERTa-77M-MLM\", chemberta_77_mlm_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemberta_77_mtr_card = ModelInfo(\n",
    "    name = \"ChemBERTa-77M-MTR\",\n",
    "    inputs = \"smiles\",\n",
    "    type=\"pretrained\",\n",
    "    group=\"huggingface\",\n",
    "    version=0,\n",
    "    submitter=\"Datamol\",\n",
    "    description=\"ChemBERTa is a pre-trained language model for molecules based on (Ro)BERT(a) trained on PubChem compounds. The MTR version was pretrained using mutitask regression objective, while the MLM version was pretrained using a masked language modeling objective\",\n",
    "    representation=\"line-notation\",\n",
    "    require_3D=False,\n",
    "    tags = [\"ChemBERTa-2\", \"smiles\",  'huggingface', \"transformers\", \"MTR\", \"RoBERTa\", \"PubChem\"],\n",
    "    authors= [\"Walid Ahmad\", \"Elana Simon\", \"Seyone Chithrananda\", \"Gabriel Grand\", \"Bharath Ramsundar\"],\n",
    "    reference = \"https://arxiv.org/abs/2209.01712\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877d3619f2924e0688222707cad224f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/17.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce248ec6292d4ddd83d0d5ebd3523c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/14.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-77M-MTR were not used when initializing RobertaForSequenceClassification: ['norm_mean', 'regression.dense.weight', 'regression.dense.bias', 'regression.out_proj.bias', 'norm_std', 'regression.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e756ed963b43c2b7c3ef04547129fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2193e0350348f2a26a0b2cb24408ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/6.96k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3b7bfec7c04b1b9f97d4debfe45dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e00bf5ee18a49cbbb1077d6be218d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/8.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d9079e8b054c54930d751964b9e283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c4d902463d4afda31a7421257ab92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/420 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b29cad3bee449d97fc037a93cf5c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 15:35:39.923 | INFO     | molfeat.trans.pretrained.hf_transformers:save:50 - Model saved to gs://molfeat-store-prod/artifacts/huggingface/ChemBERTa-77M-MTR/0/model.save\n",
      "2023-03-20 15:35:43.039 | INFO     | molfeat.store.modelstore:register:124 - Successfuly registered model ChemBERTa-77M-MTR !\n"
     ]
    }
   ],
   "source": [
    "# attempt to register the model\n",
    "model_mtr = HFModel.register_pretrained(\"DeepChem/ChemBERTa-77M-MTR\", \"DeepChem/ChemBERTa-77M-MTR\", chemberta_77_mtr_card, model_class=AutoModelForSequenceClassification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molfeat-core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd64925fe6617865d410306d2b64fa69b44b63a36aad85fd11f7d4e4dc7609f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
