{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molfeat.trans.pretrained.hf_transformers import HFExperiment\n",
    "from molfeat.trans.pretrained.hf_transformers import HFModel\n",
    "from molfeat.store import ModelInfo\n",
    "from molfeat.store import ModelStore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2 Zinc 87M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab47e0f719dc450bbc5f9b594f1767d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/788 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d548b4b992a947eb97df6079eb07d2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/350M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb24f818c1244d2b16054d4e1082c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/40.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b721a281604b481f8a65593d6b67f769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/24.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83cfa7c396624468ada9966affd68510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 09:17:47 | WARNING | google.auth._default | No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2533d8bd0147406499dfa7da59272fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 09:17:53 | WARNING | google.auth._default | No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "2023-05-14 09:17:53 | WARNING | google.auth._default | No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "2023-05-14 09:17:53 | WARNING | google.auth._default | No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "2023-05-14 09:17:53 | WARNING | google.auth._default | No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "2023-05-14 09:17:53 | WARNING | google.auth._default | No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "2023-05-14 09:17:53 | WARNING | google.auth._default | No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "2023-05-14 09:17:53 | WARNING | google.auth._default | No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "2023-05-14 09:17:53 | WARNING | google.auth._default | No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "\u001b[32m2023-05-14 09:19:08.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmolfeat.trans.pretrained.hf_transformers\u001b[0m:\u001b[36msave\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mModel saved to gs://molfeat-store-prod/artifacts/huggingface/GPT2-Zinc480M-87M/0/model.save\u001b[0m\n",
      "\u001b[32m2023-05-14 09:19:12.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmolfeat.store.modelstore\u001b[0m:\u001b[36mregister\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mSuccessfuly registered model GPT2-Zinc480M-87M !\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "molgpt2_card = ModelInfo(\n",
    "    name = \"GPT2-Zinc480M-87M\",\n",
    "    inputs = \"smiles\",\n",
    "    type=\"pretrained\",\n",
    "    group=\"huggingface\",\n",
    "    version=0,\n",
    "    submitter=\"Datamol\",\n",
    "    description=\"This is a GPT2 style autoregressive language model trained on ~480m SMILES strings from the ZINC database available. The model has ~87m parameters and was trained for 175000 iterations with a batch size of 3072 to a validation loss of ~.615.\",\n",
    "    representation=\"line-notation\",\n",
    "    require_3D=False,\n",
    "    tags = [\"smiles\", 'huggingface', \"transformers\", \"GPT2\"],\n",
    "    authors= [\"Karl Heyer\"],\n",
    "    reference = \"https://github.com/kheyer/gpt2_zinc_87m\" \n",
    ")\n",
    "\n",
    "# attempt to register the model\n",
    "model = HFModel.register_pretrained(\"entropy/gpt2_zinc_87m\", \"entropy/gpt2_zinc_87m\", molgpt2_card)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roberta Zinc 480M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10860bcb5df44f55aae7cc3db4c99bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-14 09:29:43.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmolfeat.trans.pretrained.hf_transformers\u001b[0m:\u001b[36msave\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mModel saved to gs://molfeat-store-prod/artifacts/huggingface/Roberta-Zinc480M-102M/0/model.save\u001b[0m\n",
      "\u001b[32m2023-05-14 09:29:46.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmolfeat.store.modelstore\u001b[0m:\u001b[36mregister\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mSuccessfuly registered model Roberta-Zinc480M-102M !\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "molroberta_card = ModelInfo(\n",
    "    name = \"Roberta-Zinc480M-102M\",\n",
    "    inputs = \"smiles\",\n",
    "    type=\"pretrained\",\n",
    "    group=\"huggingface\",\n",
    "    version=0,\n",
    "    submitter=\"Datamol\",\n",
    "    description=\"This is a Roberta style masked language model trained on ~480m SMILES strings from the ZINC database. The model has ~102m parameters and was trained for 150000 iterations with a batch size of 4096 to a validation loss of ~0.122.\",\n",
    "    representation=\"line-notation\",\n",
    "    require_3D=False,\n",
    "    tags = [\"smiles\", 'huggingface', \"transformers\", \"Roberta\"],\n",
    "    authors= [\"Karl Heyer\"],\n",
    "    reference = \"https://github.com/kheyer/roberta_zinc_480m\" \n",
    ")\n",
    "\n",
    "# attempt to register the model\n",
    "model = HFModel.register_pretrained(\"entropy/roberta_zinc_480m\", \"entropy/roberta_zinc_480m\", molroberta_card)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from molfeat.trans.pretrained import PretrainedHFTransformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaForMaskedLM, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"entropy/roberta_zinc_480m\", max_len=128)\n",
    "model = RobertaForMaskedLM.from_pretrained('entropy/roberta_zinc_480m')\n",
    "collator = DataCollatorWithPadding(tokenizer, padding=True, return_tensors='pt')\n",
    "\n",
    "smiles = ['Brc1cc2c(NCc3ccccc3)ncnc2s1',\n",
    " 'Brc1cc2c(NCc3ccccn3)ncnc2s1',\n",
    " 'Brc1cc2c(NCc3cccs3)ncnc2s1',\n",
    " 'Brc1cc2c(NCc3ccncc3)ncnc2s1',\n",
    " 'Brc1cc2c(Nc3ccccc3)ncnc2s1']\n",
    "\n",
    "inputs = collator(tokenizer(smiles))\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "full_embeddings = outputs[1][-1]\n",
    "mask = inputs['attention_mask']\n",
    "embeddings = ((full_embeddings * mask.unsqueeze(-1)).sum(1) / mask.sum(-1).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = PretrainedHFTransformer(\"Roberta-Zinc480M-102M\", max_len=128, layer=-1, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "molfeat_embeddings = transformer(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(molfeat_embeddings == embeddings).all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"entropy/gpt2_zinc_87m\", max_len=256)\n",
    "model = GPT2LMHeadModel.from_pretrained('entropy/gpt2_zinc_87m')\n",
    "collator = DataCollatorWithPadding(tokenizer, padding=True, return_tensors='pt')\n",
    "\n",
    "inputs = collator(tokenizer(smiles))\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "full_embeddings = outputs[-1][-1]\n",
    "mask = inputs['attention_mask']\n",
    "embeddings = ((full_embeddings * mask.unsqueeze(-1)).sum(1) / mask.sum(-1).unsqueeze(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = PretrainedHFTransformer(\"GPT2-Zinc480M-87M\", max_len=256, layer=-1, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "molfeat_embeddings = transformer(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(molfeat_embeddings == embeddings).all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, for GPT2, it's likely better to use the GPT pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-14 10:24:43.483\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmolfeat.trans.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m53\u001b[0m - \u001b[33m\u001b[1mThe 'PretrainedHFTransformer' interaction has been superseded by a new class with id 0x7f972bab9030\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "transformer = PretrainedHFTransformer(\"GPT2-Zinc480M-87M\", max_len=256, pooling=\"gpt\", layer=-1, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7721, -1.3579,  1.0738,  ..., -2.9465, -0.6193, -1.7630],\n",
       "        [-0.5820, -1.8309,  0.6659,  ..., -2.7409, -1.0297, -1.7621],\n",
       "        [-0.7349, -1.5854,  0.8986,  ..., -2.7796, -0.5494, -1.7704],\n",
       "        [-0.6369, -1.7601,  0.6885,  ..., -2.5800, -1.0640, -1.7046],\n",
       "        [-0.7219, -1.4894,  1.1671,  ..., -3.0910, -0.3174, -1.7954]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer(smiles) # which gives a different results compared to the embeddings below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3539, -1.1362, -0.8619,  ..., -1.7240, -0.0970, -0.1031],\n",
       "        [-2.1634, -1.2579, -0.8267,  ..., -1.7632,  0.0584, -0.2343],\n",
       "        [-2.2977, -1.1809, -0.7014,  ..., -1.8313,  0.1073, -0.2144],\n",
       "        [-2.2383, -1.0197, -0.8010,  ..., -1.7786, -0.0261, -0.1898],\n",
       "        [-2.3104, -1.1759, -0.5705,  ..., -1.4688, -0.1083, -0.2210]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molfeat-core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd64925fe6617865d410306d2b64fa69b44b63a36aad85fd11f7d4e4dc7609f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
